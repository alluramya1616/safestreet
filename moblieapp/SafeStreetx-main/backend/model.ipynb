{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.image as mpimg\n",
    "from nltk import pos_tag\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, add, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from datasetLoader import datasetLoader\n",
    "\n",
    "import Model from Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(\"data_results.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns with a new mapping\n",
    "columns_mapping = {\n",
    "    \" comment_number\" : \"comment_number\",\n",
    "    \" comment\" : \"comment\"\n",
    "}\n",
    "dataframe = df.rename(columns=columns_mapping)\n",
    "\n",
    "# Group data by image name and aggregate comments into lists\n",
    "grouped = dataframe.groupby('image_name')['comment'].apply(lambda x: x.tolist()).reset_index(name='comments')\n",
    "\n",
    "# Generate image paths\n",
    "image_file_paths = ['/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/' + img_name for img_name in grouped['image_name']]\n",
    "captions = grouped['comments']\n",
    "\n",
    "# Calculate unique counts and total records\n",
    "num_unique_images = dataframe['image_name'].nunique()\n",
    "num_unique_comments = dataframe['comment'].nunique()\n",
    "num_total_records = len(dataframe)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of unique images: {num_unique_images}\")\n",
    "print(f\"Number of unique comments: {num_unique_comments}\")\n",
    "print(f\"Total number of records: {num_total_records}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def preprocess_text(dataset):\n",
    "    # Clean the captions in the dataset\n",
    "    dataset['description'] = dataset['description'].apply(lambda text: text.lower())\n",
    "    dataset['description'] = dataset['description'].apply(lambda text: text.replace(\"[^A-Za-z]\", \"\"))\n",
    "    dataset['description'] = dataset['description'].apply(lambda text: text.replace(\"\\s+\", \" \"))\n",
    "    dataset['description'] = dataset['description'].apply(lambda text: \" \".join([word for word in text.split() if len(word) > 1]))\n",
    "    dataset['description'] = \"startseq \" + dataset['description'] + \" endseq\"  # Add startseq and endseq\n",
    "    return dataset\n",
    "\n",
    "\n",
    "processed_data = preprocess_text(dataframe)\n",
    "descriptions_list = processed_data['description'].tolist()\n",
    "\n",
    "\n",
    "text_tokenizer = Tokenizer()\n",
    "text_tokenizer.fit_on_texts(descriptions_list)\n",
    "vocabulary_size = len(text_tokenizer.word_index) + 1\n",
    "print(vocabulary_size)\n",
    "\n",
    "max_caption_length = max(len(description.split()) for description in descriptions_list)\n",
    "\n",
    "image_files = processed_data['image'].unique().tolist()\n",
    "total_images = len(image_files)\n",
    "\n",
    "\n",
    "split_idx = round(0.7 * total_images)\n",
    "train_image_files = image_files[:split_idx]\n",
    "test_image_files = image_files[split_idx:]\n",
    "\n",
    "train_set = processed_data[processed_data['image'].isin(train_image_files)]\n",
    "test_set = processed_data[processed_data['image'].isin(test_image_files)]\n",
    "\n",
    "\n",
    "train_set.reset_index(inplace=True, drop=True)\n",
    "test_set.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator, test_generator, validation_generator = datasetLoader(train_set, test_set, 50, 3)\n",
    "caption_model = Model(\"clip\")\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "caption_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = caption_model.fit(\n",
    "        train_generator,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "caption_model.save_weights('model.h5')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
